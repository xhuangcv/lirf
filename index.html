<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>LIRF</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <h2>Local Implicit Ray Function for Generalizable Radiance Field Representation</h2>
            <h4 style="color:#5a6268;">CVPR 2023</h4>
            <hr>
            <h6>
              <a href="https://github.com/shsf0817/" target="_blank">Xin Huang</a><sup>1</sup>, 
              <a href="https://qzhang-cv.github.io/" target="_blank">Qi Zhang</a><sup>2</sup>, 
              <a href="" target="_blank">Ying Feng</a><sup>2</sup>,
              <a href="https://xiaoyu258.github.io/" target="_blank">Xiaoyu Li</a><sup>2</sup>,
              <a href="https://xuanwangvc.github.io/" target="_blank">Xuan Wang</a><sup>2</sup>,
              <a href="https://teacher.nwpu.edu.cn/qwang.html" target="_blank">Qing Wang</a><sup>1</sup>
            </h6>
            <p><sup>1</sup>Northwestern Polytechnical University &nbsp;&nbsp; 
                <sup>2</sup>Tencent AI Lab
            <br>

            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Paper</a> </p>
              </div>
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="" role="button"  target="_blank">
                  <i class="fa fa-file"></i> arXiv</a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="" role="button" target="_blank">
                    <i class="fa fa-github-alt"></i> Code (Comming Soon)</a> </p>
              </div>
<!--               <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="" role="button">
                    <i class="fa fa-file"></i> Supplementary</a> </p>
              </div> -->
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
            <hr style="margin-top:0px">
                <img class="img-fluid" src="images/overview.png" alt="Architechture">
            <hr>
          <p class="text-justify"> 
            We propose <b>LIRF</b> (Local Implicit Ray Function), a generalizable neural rendering approach for novel view rendering. 
            Current generalizable neural radiance fields (NeRF) methods sample a scene with a single ray per pixel and may therefore 
            render blurred or aliased views when the input views and rendered views capture scene content with different resolutions. 
            To solve this problem, we propose LIRF to aggregate the information from conical frustums to construct a ray. Given 3D 
            positions within conical frustums, LIRF takes 3D coordinates and the features of conical frustums as inputs and predicts 
            a local volumetric radiance field. Since the coordinates are continuous, LIRF renders high-quality novel views at a 
            continuously-valued scale via volume rendering.  Besides, we predict the visible weights for each input view via 
            transformer-based feature matching to improve the performance in occluded areas. Experimental results on real-world 
            scenes validate that our method outperforms state-of-the-art methods on novel view rendering of unseen scenes at arbitrary scales. 
          </p> 
        </div>
      </div>
    </div>
  </section>
  <br>

    <!-- pipeline -->
    <section>
      <div class="container">
        <div class="row">
          <div class="col-12 text-center">
              <h3>Pipeline Overview</h3>
              <hr style="margin-top:0px">
                  <img class="img-fluid" src="images/pipeline.png" alt="Architechture">
                  <!-- <img class="img-fluid" src="images/pipeline.png" alt="Architechture" style="width:512px;"> -->
              <hr>
              <p class="text-justify">
                The overview of LIRF. Our goal is to predict volumetric radiance fields from a set of multi-view images captured at a consistent image scale (&times1), 
                and output novel views at continuous scales (&times;0.5 &sim; &times;4). Our proposed framework is composed of five parts: 
                1) extracting 2D feature maps from source images, 
                2) obtaining the image feature for the samples on target rays via local implicit ray function, 
                3) predicting the visibility weights of each source view by matching feature patches, 
                4) aggregating local image features from different source views and mapping them into colors and densities, 
                5) rendering a target pixel via volume rendering.
              </p>
          </div>
        </div>
      </div>
    </section>
    <br>

  <!-- Multiscale views -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Low Resolution Views and Close-up Shots</h3>
            <p class="text-justify">
              We train LIRF on datasets with input images at &times1 resolution and GT images at different resolutions (&times;0.5 &sim; &times;4). 
              LIRF is capable of rendering high-fidelity views at varying scales. 
              Comparied with <a href="https://ibrnet.github.io/">IBRNet</a>, <a href="https://liuyuan-pal.github.io/NeuRay/">NeuRay</a>, and <a href="https://www.idiap.ch/paper/geonerf/">GeoNeRF</a>, 
              LIRF produces Low Resolution Views (&times;0.5) with fewer aliasing artifacts and Close-up Shots (&times;2) with fewer blurring artifacts.
            </p>
            <hr style="margin-top:0px">
              <div class="cropped">
                <video width="90%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                <source src="images/video1.mp4" type="video/mp4">
              </div>
    
              <div class="cropped">
              <video width="90%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                  <source src="images/video2.mp4" type="video/mp4">
              </video>
              </div>

              <div class="cropped">
              <video width="90%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                  <source src="images/video3.mp4" type="video/mp4">
              </video>
              </div>
        </div>
      </div>
    </div>
  </section>
  <br>

    <!-- Our views -->
    <section>
      <div class="container">
        <div class="row">
          <div class="col-12 text-center">
              <h3>Varying Resolutions</h3>
              <p class="text-justify">
               LIRF is a novel method for novel view synthesis of unseen scenes. It not only renders novel views with fewer blurring artifacts, but also produces novel views at arbitrary scales, even at higher scales than input views.
              </p>
              <hr style="margin-top:0px">
              <video width="90%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                  <source src="images/video4.mp4" type="video/mp4">
              </video>

          </div>
        </div>
      </div>
    </section>
    <br>


  <!-- citing -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@inproceedings{huang2023lirf,
    title={Local Implicit Ray Function for Generalizable Radiance Field Representation},
    author={Xin, Huang and Qi, Zhang and Ying, Feng and Xiaoyu, Li and Xuan, Wang and Qing, Wang},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    year={2023}
}</code></pre>
          <hr>
      </div>
    </div>
  </div>

  <!-- ack -->
  <!-- <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Acknowledgements</h3>
          <hr style="margin-top:0px">
          <p class="text-justify">
          We thank Li Ma and Xiaoyu Li for their instructive and useful advices.
          </p>
          <hr>
      </div>
    </div>
  </div> -->

  <footer class="text-center" style="margin-bottom:10px">
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the website template.
  </footer>

  <script>
    MathJax = {
      tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-713SR7NR2X"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-713SR7NR2X');
  </script>

</body>
</html>
